import requests
import re
import urllib3
from bs4 import BeautifulSoup

# SSL uyarılarını kapat (Güvenli olmayan bağlantı hatasını engellemek için)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Daha gerçekçi bir tarayıcı kimliği
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
    "Referer": "https://www.google.com/"
}

NEONSPOR_URL = "https://raw.githubusercontent.com/primatzeka/kurbaga/main/NeonSpor/NeonSpor.m3u"

def find_active_site():
    print("Aktif TRGoals sitesi aranıyor...")
    # Aralığı 1490'dan başlatıp 1600'e kadar tarıyoruz
    for i in range(1490, 1601):
        url = f"https://trgoals{i}.xyz"
        try:
            # verify=False ekledik: SSL hatası olsa bile sayfayı açar
            r = requests.get(url, headers=HEADERS, timeout=5, verify=False, allow_redirects=True)
            if r.status_code == 200:
                print(f"[+] Aktif site bulundu: {url}")
                return url
        except Exception:
            continue
    return None

def clean_channel_name(text):
    return re.sub(r"\s+", " ", text).strip()

def get_channel_data(active_url):
    channel_map = {}
    base_url_found = "https://ogr.d72577a9dd0ec6.sbs/" # Varsayılan/Yedek URL

    try:
        r = requests.get(active_url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(r.text, "html.parser")

        # Kanal listesini çekme (Daha esnek hale getirildi)
        # Sitede 'a' etiketleri içinde 'id=' olanları bulur
        items = soup.find_all("a", href=re.compile(r"id="))
        
        for item in items:
            href = item.get("href", "")
            # Kanal adını 'channel-name' divinden veya doğrudan text'ten al
            name_div = item.select_one("div.channel-name")
            channel_name = clean_channel_name(name_div.get_text()) if name_div else clean_channel_name(item.get_text())
            
            id_match = re.search(r"id=([a-zA-Z0-9_]+)", href)
            if id_match:
                channel_id = id_match.group(1)
                if channel_name:
                    channel_map[channel_id] = channel_name

        # Yayın kaynağını (baseurl) çekme
        # Sayfa içindeki scriptleri tara
        test = requests.get(f"{active_url}/channel.html?id=yayin1", headers=HEADERS, timeout=5, verify=False)
        m = re.search(r'const\s+baseurl\s*=\s*"([^"]+)"', test.text)
        if m:
            base_url_found = m.group(1)
            print(f"[i] Yayın kaynağı güncellendi: {base_url_found}")

    except Exception as e:
        print(f"[-] Veri çekme hatası: {e}")

    return channel_map, base_url_found

def fetch_neonspor():
    try:
        r = requests.get(NEONSPOR_URL, timeout=10)
        lines = []
        for line in r.text.splitlines():
            if line.startswith("#EXTINF"):
                if 'group-title=' not in line:
                    line = line.replace("#EXTINF:-1", '#EXTINF:-1 group-title="NeonSpor"')
            if not line.startswith("#EXTM3U"):
                lines.append(line)
        return lines
    except:
        print("[-] NeonSpor listesi alınamadı.")
        return []

def create_m3u():
    active_site = find_active_site()
    if not active_site:
        print("[-] Aktif TRGoals sitesi bulunamadı. Lütfen internetinizi veya domain aralığını kontrol edin.")
        return

    channel_map, base_url = get_channel_data(active_site)
    if not channel_map:
        print("[-] TRGoals kanal listesi çekilemedi (Sayfa yapısı değişmiş olabilir).")
        return

    m3u = ["#EXTM3U"]

    # ---------- TRGOALS ----------
    for cid, name in channel_map.items():
        stream = f"{base_url}{cid}.m3u8"
        m3u.append(f'#EXTINF:-1 group-title="TRGoals",{name}')
        m3u.append(f'#EXTVLCOPT:http-referrer={active_site}/')
        m3u.append(f'#EXTVLCOPT:http-origin={active_site}')
        m3u.append(f'#EXTVLCOPT:http-user-agent={HEADERS["User-Agent"]}')
        m3u.append(stream)

    # ---------- NEONSPOR ----------
    m3u.extend(fetch
